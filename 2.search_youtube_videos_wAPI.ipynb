{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Program to search YouTube using its API\n",
    "(by Valentin Todorov - 08/28/2017)\n",
    "<n>\n",
    "This program was developed to scrub all videos from Suzanita & Kaskata in which \"Allahumma Ya Subuhun\" can be heard in the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import the libraries that will be used in the program\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# My API key from YouTube\n",
    "api_key = \"AIzaSyDHXROE2Qt0NglgCmTnO8Y8cMikF5mD-m4\"\n",
    "\n",
    "# Set the parameters for the number of search pages to return, the number of records per search page and the location of the final Excel file\n",
    "search_pages = 10\n",
    "search_results_per_page = 45\n",
    "final_output_location = \"/Users/valentin/Documents/VideoSearch\"\n",
    "\n",
    "\n",
    "# Initialize empty lists for the data I'll be collecting - videos' links, titles, uploader, channel, and date uploaded on\n",
    "video_link = []\n",
    "video_title = []\n",
    "video_user = []\n",
    "video_user_channel = []\n",
    "video_publish_date = []\n",
    "search_term_used = []\n",
    "\n",
    "\n",
    "# Search terms: suzanita+lucifer; suzanita+kaskata; lucifer+buddha; луцифер+буда; сузанита+луцифер; сузанита+каската; biz+amulet;\n",
    "# Define the parameters\n",
    "# For search terms in Bulgarian only -->>  (1) In \"search_term\" write the search in Bulgarian, and (2) in \"search_term_save\" write the search in English\n",
    "search_term = [\"suzanita+lucifer\", \"suzanita+kaskata\", \"lucifer+buddha\", \"biz+amulet\", \"сузанита+луцифер\", \"сузанита+каската\"]\n",
    "search_term_save = [\"suzanita+lucifer\", \"suzanita+kaskata\", \"lucifer+buddha\", \"biz+amulet\", \"suzanita+lucifer_inBulgarian\", \"suzanita+kaskata_inBulgarian\"]\n",
    "\n",
    "\n",
    "for s in range(len(search_term)):\n",
    "    \n",
    "    # Create an empty token value for the first page - Each search page returned has a token\n",
    "    # To go to the next page with results we need to add the token value to the end of the url\n",
    "    # The token value will be updated from the JSON file which is returned in the API call\n",
    "    next_page_token = \"\"\n",
    "\n",
    "    print (\"\\nBeginning the search for: \" + search_term_save[s])\n",
    "    time.sleep(1)\n",
    "\n",
    "    # Loop through the first n pages of search results\n",
    "    for page_results in range(1, (search_pages + 1)):\n",
    "\n",
    "        # API call and JSON response\n",
    "        search_url = (\"https://www.googleapis.com/youtube/v3/search?part=snippet&maxResults=\" + str(search_results_per_page) + \"&q=\" + search_term[s] + \"&key=\" + api_key + \"&pageToken=\" + next_page_token)\n",
    "        api_response = requests.get(url = search_url)\n",
    "        json_data = json.loads(api_response.text)\n",
    "\n",
    "        if page_results == 1:\n",
    "            print (\"The program will collect the information from the first \" + str(search_pages * search_results_per_page) + \" videos returned in the search\")\n",
    "            time.sleep(1)\n",
    "            print (\"The total number of videos found on YouTube for the search term \" + search_term[s] + \" are: \" + str(json_data[\"pageInfo\"][\"totalResults\"]))\n",
    "\n",
    "        # Update the token for the next page with search results\n",
    "        next_page_token = str(json_data[\"nextPageToken\"])\n",
    "\n",
    "        print (\"Extracting content from page: \" + str(page_results))\n",
    "        print (\"The next page token is: \" + str(next_page_token))\n",
    "        print search_url\n",
    "\n",
    "        # Loop through all the search results returned in the API\n",
    "        for videos in range(0, len(json_data[\"items\"])):\n",
    "\n",
    "            # Get the link to video\n",
    "            video_link.append(\"https://www.youtube.com/watch?v=\" + json_data[\"items\"][videos][\"id\"].values()[1])\n",
    "\n",
    "            # Get title of video\n",
    "            video_title.append(json_data[\"items\"][videos][\"snippet\"][\"title\"])\n",
    "\n",
    "            # Get the user name/channel name from which a video was uploaded\n",
    "            video_user.append(json_data[\"items\"][videos][\"snippet\"][\"channelTitle\"])\n",
    "\n",
    "            # Get link to the channel\n",
    "            video_user_channel.append(\"https://www.youtube.com/channel/\" + json_data[\"items\"][videos][\"snippet\"][\"channelId\"])\n",
    "\n",
    "            # Date of video upload. The extract date is a string, from which the data is extracted\n",
    "            # Example from here: https://stackoverflow.com/questions/37192942/extract-date-from-string-in-python\n",
    "            string_date = str(json_data[\"items\"][videos][\"snippet\"][\"publishedAt\"])\n",
    "            match = re.search(r\"\\d{4}-\\d{2}-\\d{2}\", string_date)\n",
    "            date = datetime.strptime(match.group(), \"%Y-%m-%d\").date()\n",
    "            video_publish_date.append(date)\n",
    "\n",
    "            # Add the search term that was used\n",
    "            search_term_used.append(search_term_save[s])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Create a dataframe with the video links, titles, username and number of views\n",
    "final_combined_df = pd.DataFrame(list(zip(video_link, video_title, video_user, video_user_channel, video_publish_date, search_term_used)),\n",
    "                                 columns = [\"video_link\", \"video_title\", \"video_user\", \"video_user_channel\", \"video_publish_date\", \"search_term_used\"])\n",
    "\n",
    "# Dedup the dataframe by video_link\n",
    "# YouTube returns the same video in the search results when using different search terms)\n",
    "final_combined_df2 = final_combined_df.drop_duplicates(subset = [\"video_link\"], keep = \"first\")\n",
    "\n",
    "# Remove rows where len(video_link) > 43. Playlists and channels have a url length greater than 43\n",
    "# These can also be removed through the API -> (json_data[\"items\"][tt][\"id\"].values()[0] == \"youtube#video\")\n",
    "final_combined_df3 = final_combined_df2[final_combined_df2[\"video_link\"].map(len) < 44]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2498, 6), (1847, 6), (1287, 6))"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the sizes of the dataframes\n",
    "final_combined_df.shape, final_combined_df2.shape, final_combined_df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Get the duration for each video and append it to the Pandas dataframe\n",
    "\n",
    "# Remove playlists from the list ->> keep only elements in the list with length < 44\n",
    "video_link = list(set(video_link))\n",
    "video_link = [x for x in video_link if len(x) < 44]\n",
    "\n",
    "# YouTube only allows API requests for up to 50 videos at a time. I created buckets with 40 videos in each\n",
    "video_link3 = np.array_split(video_link, len(video_link)/40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1287, 32)"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some more checking\n",
    "len(list(set(video_link))), len(video_link3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Run through the list with videos, extract the duration and output the information\n",
    "video_link = []\n",
    "video_duration_str = []\n",
    "\n",
    "for i in range(0, len(video_link3)):\n",
    "    videos_list_search = \"%2C+\".join([x.split(\"?v=\")[1] for x in video_link3[i]])\n",
    "    \n",
    "    search_url = \"https://www.googleapis.com/youtube/v3/videos?part=snippet%2CcontentDetails%2Cstatistics&id=\" + videos_list_search + \"&key=\" + api_key\n",
    "    api_response = requests.get(url = search_url)\n",
    "    json_data = json.loads(api_response.text)\n",
    "    \n",
    "    video_index = len(json_data[\"items\"])\n",
    "    video_link.append([\"https://www.youtube.com/watch?v=\" + json_data[\"items\"][x][\"id\"] for x in range(0, video_index)])\n",
    "    video_duration_str.append([json_data[\"items\"][x][\"contentDetails\"][\"duration\"].split(\"PT\")[1] for x in range(0, video_index)])\n",
    "\n",
    "video_link = [x for sublist in video_link for x in sublist]\n",
    "video_duration_str = [x for sublist in video_duration_str for x in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Merge the video durations with the list of videos found in the search\n",
    "video_duration_df = pd.DataFrame(list(zip(video_link, video_duration_str)),\n",
    "                                 columns = [\"video_link\", \"video_duration_str\"])\n",
    "\n",
    "final_combined_df4 = pd.merge(final_combined_df3, video_duration_df, on = \"video_link\", how = \"left\")\n",
    "\n",
    "## Write to an Excel file on my computer\n",
    "print (\"\\nWrite file to Excel on Google Drive\\n The location is: \"  + final_output_location + \"/videos_links_final_\" + time.strftime(\"%Y%m%d\") + \".xlsx\")\n",
    "\n",
    "writer_excel = pd.ExcelWriter(final_output_location + \"/videos_links_final_\" + time.strftime(\"%Y%m%d\") + \".xlsx\")\n",
    "final_combined_df4.to_excel(writer_excel, 'Sheet1')\n",
    "writer_excel.save()\n",
    "\n",
    "print (\"\\nComplete!\\nFile successfully written to Google Drive\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable                  Type           Data/Info\n",
      "--------------------------------------------------\n",
      "api_key                   str            AIzaSyDHXROE2Qt0NglgCmTnO8Y8cMikF5mD-m4\n",
      "api_response              Response       <Response [200]>\n",
      "date                      date           2017-08-04\n",
      "datetime                  type           <type 'datetime.datetime'>\n",
      "final_combined_df         DataFrame                               <...>\\n[2498 rows x 6 columns]\n",
      "final_combined_df2        DataFrame                               <...>\\n[1847 rows x 6 columns]\n",
      "final_combined_df3        DataFrame                               <...>\\n[1287 rows x 6 columns]\n",
      "final_combined_df4        DataFrame                               <...>\\n[1287 rows x 7 columns]\n",
      "final_combined_df5        DataFrame                               <...>\\n[1050 rows x 8 columns]\n",
      "final_output_location     str            /Users/valentin/Documents/VideoSearch\n",
      "flattened                 list           n=22\n",
      "i                         int            31\n",
      "json                      module         <module 'json' from '/Use<...>on2.7/json/__init__.pyc'>\n",
      "json_data                 dict           n=4\n",
      "match                     SRE_Match      <_sre.SRE_Match object at 0x1180ceb28>\n",
      "next_page_token           str            CMIDEAA\n",
      "np                        module         <module 'numpy' from '/Us<...>ages/numpy/__init__.pyc'>\n",
      "page_results              int            10\n",
      "pd                        module         <module 'pandas' from '/U<...>ges/pandas/__init__.pyc'>\n",
      "re                        module         <module 're' from '/Users<...>27/lib/python2.7/re.pyc'>\n",
      "requests                  module         <module 'requests' from '<...>s/requests/__init__.pyc'>\n",
      "s                         int            5\n",
      "search_pages              int            10\n",
      "search_results_per_page   int            45\n",
      "search_term               list           n=6\n",
      "search_term_save          list           n=6\n",
      "search_term_used          list           n=2498\n",
      "search_url                unicode        https://www.googleapis.co<...>t0NglgCmTnO8Y8cMikF5mD-m4\n",
      "string_date               str            2017-08-04T04:11:53.000Z\n",
      "sublist                   list           n=40\n",
      "textwrap                  module         <module 'textwrap' from '<...>/python2.7/textwrap.pyc'>\n",
      "time                      module         <module 'time' from '/Use<...>2.7/lib-dynload/time.so'>\n",
      "tt                        DataFrame                               <...>    PT7S             7S  \n",
      "val                       unicode        PT4M7S\n",
      "video_duration            list           n=0\n",
      "video_duration_df         DataFrame                               <...>\\n[1287 rows x 2 columns]\n",
      "video_duration_min        list           n=50\n",
      "video_duration_sec        int            30\n",
      "video_duration_str        list           n=1287\n",
      "video_id                  list           n=22\n",
      "video_index               int            40\n",
      "video_link                list           n=1287\n",
      "video_link2               list           n=50\n",
      "video_link3               list           n=32\n",
      "video_link4               ndarray        10: 10 elems, type `<U43`, 1720 bytes\n",
      "video_publish_date        list           n=2498\n",
      "video_title               list           n=2498\n",
      "video_user                list           n=2498\n",
      "video_user_channel        list           n=2498\n",
      "videos                    int            42\n",
      "videos_chunk              int            82976\n",
      "videos_list_search        unicode        MxEHaSzRNAs%2C+qbUgtM2UpQ<...>zX_yiWWkNs%2C+JJhkC_nscQc\n",
      "videos_list_search2       unicode        J30UOKAD4jw%2C+egTZVzh8wt<...>0%2C+8S8bHhXjqVw%2C+8Lw3x\n",
      "writer_excel              _XlsxWriter    <pandas.io.excel._XlsxWri<...>er object at 0x117328b90>\n",
      "x                         unicode        42M2S\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
