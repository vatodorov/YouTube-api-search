{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Program to search YouTube using its API\n",
    "(by Valentin Todorov - 11/28/2017)\n",
    "<br>\n",
    "This program was developed to find all music videos by Suzanita & Kaskata in YouTube which contain a copyrighted version of the song \"Allahumma Ya Subuhun\". Using pre-defined search terms, the video links are collected and sent to YouTube's copyright violations team to remove the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import the libraries that will be used in the program\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Allocate values to variables\n",
    "api_key = \"\"         # Paste YouTube's API key. TO DO: Create an environment setup file and add it to .gitignore. Use configparser to setup environment\n",
    "search_url_prefix = \"https://www.googleapis.com/youtube/v3/search?part=snippet&maxResults=\"\n",
    "video_url_prefix = \"https://www.youtube.com/watch?v=\"\n",
    "channel_url_prefix = \"https://www.youtube.com/channel/\"\n",
    "api_url_prefix = \"https://www.googleapis.com/youtube/v3/videos?part=snippet%2CcontentDetails%2Cstatistics&id=\"\n",
    "    \n",
    "# Set the parameters for the number of search pages to return, the number of records per search page and the location of the final Excel file\n",
    "search_pages = 10\n",
    "search_results_per_page = 50\n",
    "final_output_location = \"/Users/valentin/Documents/VideoSearch\"\n",
    "\n",
    "# Search terms: suzanita+lucifer; suzanita+kaskata; lucifer+buddha; луцифер+буда; сузанита+луцифер; сузанита+каската; biz+amulet;\n",
    "# Define the parameters\n",
    "# For search terms in Bulgarian only -->>  (1) In \"search_term\" write the search in Bulgarian, and (2) in \"search_term_save\" write the search in English\n",
    "search_term = [\"suzanita+lucifer\", \"suzanita+kaskata\", \"lucifer+buddha\", \"biz+amulet\", \"сузанита+луцифер\", \"сузанита+каската\", \"луцифер+буда\"]\n",
    "search_term_save = [\"suzanita+lucifer\", \"suzanita+kaskata\", \"lucifer+buddha\", \"biz+amulet\", \"suzanita+lucifer_bg\", \"suzanita+kaskata_bg\", \"lucifer+buda_bg\"]\n",
    "\n",
    "\n",
    "# Initialize empty lists for the data I'll be collecting - videos' links, titles, uploader, uploader channel, date uploaded on and search term used\n",
    "video_link = []\n",
    "video_title = []\n",
    "video_user = []\n",
    "video_user_channel = []\n",
    "video_publish_date = []\n",
    "search_term_used = []\n",
    "\n",
    "for s in range(len(search_term)):\n",
    "    search_pages = 10\n",
    "    \n",
    "    # Create an empty token value for the first page - Each search page returned has a token\n",
    "    # To go to the next page with results we need to add the token value to the end of the url\n",
    "    # The token value will be updated from the JSON file which is returned in the API call\n",
    "    next_page_token = \"\"\n",
    "\n",
    "    print (\"\\nBeginning the search for: \" + search_term_save[s])\n",
    "    time.sleep(1)\n",
    "\n",
    "    # Get the total number of pages with results and loop through the first n pages\n",
    "    api_response = requests.get(url = (search_url_prefix + str(search_results_per_page) + \"&q=\" + search_term[s] + \"&key=\" + api_key))\n",
    "    total_pages = int(json.loads(api_response.text)[\"pageInfo\"][\"totalResults\"] / search_results_per_page)\n",
    "\n",
    "    if search_pages > total_pages:\n",
    "        search_pages = total_pages\n",
    "        \n",
    "    for page_results in range(1, (search_pages + 1)):\n",
    "\n",
    "        # API call and JSON response\n",
    "        search_url = (search_url_prefix + str(search_results_per_page) + \"&q=\" + search_term[s] + \"&key=\" + api_key + \"&pageToken=\" + next_page_token)\n",
    "        api_response = requests.get(url = search_url)\n",
    "        json_data = json.loads(api_response.text)\n",
    "\n",
    "        if page_results == 1:\n",
    "            print (\"The program will collect the information from the first \" + str(search_pages * search_results_per_page) + \" videos returned in the search\")\n",
    "            time.sleep(1)\n",
    "            print (\"The total number of videos found on YouTube for the search term \" + search_term[s] + \" are: \" + str(json_data[\"pageInfo\"][\"totalResults\"]))\n",
    "\n",
    "        # Update the token for the next page with search results\n",
    "        next_page_token = str(json_data[\"nextPageToken\"])\n",
    "\n",
    "        print (\"Extracting content from page: \" + str(page_results))\n",
    "        print (\"The next page token is: \" + str(next_page_token))\n",
    "        print search_url\n",
    "\n",
    "        # Loop through all the search results returned in the API\n",
    "        for videos in range(0, len(json_data[\"items\"])):\n",
    "\n",
    "            # Get the link to video\n",
    "            video_link.append(video_url_prefix + json_data[\"items\"][videos][\"id\"].values()[1])\n",
    "\n",
    "            # Get title of video\n",
    "            video_title.append(json_data[\"items\"][videos][\"snippet\"][\"title\"])\n",
    "\n",
    "            # Get the user name/channel name from which a video was uploaded\n",
    "            video_user.append(json_data[\"items\"][videos][\"snippet\"][\"channelTitle\"])\n",
    "\n",
    "            # Get link to the channel\n",
    "            video_user_channel.append(channel_url_prefix + json_data[\"items\"][videos][\"snippet\"][\"channelId\"])\n",
    "\n",
    "            # Date of video upload. The extract date is a string, from which the data is extracted\n",
    "            # Example from here: https://stackoverflow.com/questions/37192942/extract-date-from-string-in-python\n",
    "            string_date = str(json_data[\"items\"][videos][\"snippet\"][\"publishedAt\"])\n",
    "            match = re.search(r\"\\d{4}-\\d{2}-\\d{2}\", string_date)\n",
    "            date = datetime.strptime(match.group(), \"%Y-%m-%d\").date()\n",
    "            video_publish_date.append(date)\n",
    "\n",
    "            # Add the search term that was used\n",
    "            search_term_used.append(search_term_save[s])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a dataframe with the video links, titles, username and number of views\n",
    "final_combined_df = pd.DataFrame(list(zip(video_link, video_title, video_user, video_user_channel, video_publish_date, search_term_used)),\n",
    "                                 columns = [\"video_link\", \"video_title\", \"video_user\", \"video_user_channel\", \"video_publish_date\", \"search_term_used\"])\n",
    "\n",
    "# Dedup the dataframe by video_link\n",
    "# YouTube returns the same video in the search results when using different search terms)\n",
    "final_combined_df2 = final_combined_df.drop_duplicates(subset = [\"video_link\"], keep = \"first\")\n",
    "\n",
    "# Remove rows where len(video_link) > 43. Playlists and channels have a url length greater than 43\n",
    "# These can also be removed through the API -> (json_data[\"items\"][tt][\"id\"].values()[0] == \"youtube#video\")\n",
    "final_combined_df3 = final_combined_df2[final_combined_df2[\"video_link\"].map(len) < 44]\n",
    "\n",
    "# Remove videos that have already been marked as non-cases during a manual search\n",
    "reviewed_videos = pd.read_excel(final_output_location + \"/reviewed_videos_important.xlsx\")\n",
    "final_combined_df3 = pd.merge(final_combined_df3, reviewed_videos[[\"video_link\", \"keep\"]], on = \"video_link\", how = \"left\")\n",
    "final_combined_df3 = final_combined_df3[final_combined_df3[\"keep\"] != \"d\"]\n",
    "final_combined_df3 = final_combined_df3.drop(\"keep\", 1)\n",
    "\n",
    "# Check the sizes of the dataframes\n",
    "final_combined_df.shape, final_combined_df2.shape, final_combined_df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the duration for each video and append it to the Pandas dataframe\n",
    "\n",
    "# Remove playlists from the list ->> keep only elements in the list with length < 44\n",
    "video_link = list(set(video_link))\n",
    "video_link = [x for x in video_link if len(x) < 44]\n",
    "\n",
    "# YouTube only allows API requests for up to 50 videos at a time. I created buckets with 40 videos in each\n",
    "video_link3 = np.array_split(video_link, len(video_link)/40)\n",
    "\n",
    "# Some more checking\n",
    "len(list(set(video_link))), len(video_link3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Run through the list with videos, extract the duration and output the information\n",
    "video_link = []\n",
    "video_duration_str = []\n",
    "\n",
    "for i in range(0, len(video_link3)):\n",
    "    videos_list_search = \"%2C+\".join([x.split(\"?v=\")[1] for x in video_link3[i]])\n",
    "    \n",
    "    search_url = api_url_prefix + videos_list_search + \"&key=\" + api_key\n",
    "    api_response = requests.get(url = search_url)\n",
    "    json_data = json.loads(api_response.text)\n",
    "    \n",
    "    video_index = len(json_data[\"items\"])\n",
    "    video_link.append([video_url_prefix + json_data[\"items\"][x][\"id\"] for x in range(0, video_index)])\n",
    "    video_duration_str.append([json_data[\"items\"][x][\"contentDetails\"][\"duration\"].split(\"PT\")[1] for x in range(0, video_index)])\n",
    "\n",
    "video_link = [x for sublist in video_link for x in sublist]\n",
    "video_duration_str = [x for sublist in video_duration_str for x in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merge the video durations with the list of videos found in the search\n",
    "video_duration_df = pd.DataFrame(list(zip(video_link, video_duration_str)),\n",
    "                                 columns = [\"video_link\", \"video_duration_str\"])\n",
    "\n",
    "final_combined_df4 = pd.merge(final_combined_df3, video_duration_df, on = \"video_link\", how = \"left\")\n",
    "\n",
    "# Remove videos published prior to 2017-06-01\n",
    "final_combined_df4[\"video_publish_date\"] = pd.to_datetime(final_combined_df4[\"video_publish_date\"])\n",
    "mask = (final_combined_df4[\"video_publish_date\"] >= \"2017-06-01\")\n",
    "final_combined_df4 = final_combined_df4.loc[mask]\n",
    "\n",
    "# Remove videos longer than 59 minutes. Need to make this to remove videos longer than 15 minutes (there are ~ 100 that are 15 min and longer)\n",
    "final_combined_df4 = final_combined_df4[final_combined_df4.video_duration_str.str.contains(\"H\") == False]\n",
    "#final_combined_df4 = final_combined_df4[\"B\"].str.split(\"M\", expand = True)[0]    # Remove videos > 15 minutes\n",
    "\n",
    "## Write to an Excel file on my computer\n",
    "print (\"\\nWrite file to Excel on Google Drive\\n The location is: \"  + final_output_location + \"/videos_links_final_\" + time.strftime(\"%Y%m%d\") + \".xlsx\")\n",
    "\n",
    "writer_excel = pd.ExcelWriter(final_output_location + \"/videos_links_final_\" + time.strftime(\"%Y%m%d\") + \".xlsx\")\n",
    "final_combined_df4.to_excel(writer_excel, 'Sheet1')\n",
    "writer_excel.save()\n",
    "\n",
    "print (final_combined_df4.shape)\n",
    "print (\"\\nComplete!\\nFile successfully written to Google Drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
