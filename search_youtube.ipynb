{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# suzanita+lucifer; suzanita+kaskata; луцифер+буда; сузанита+луцифер; amulet+biz;\n",
    "\n",
    "# Include the search term. Multiple terms should be separated with a \"+\" (no spaces)\n",
    "textToSearch = \"сузанита+луцифер\"\n",
    "\n",
    "# Initialize empty lists for the data I'll be collecting - videos' links, titles, uploader, and number of days since upload\n",
    "video_link = []\n",
    "video_title = []\n",
    "video_user = []\n",
    "uploaded_since = []\n",
    "video_views = []\n",
    "search_term = []\n",
    "\n",
    "\n",
    "## Search loop\n",
    "# This should eventually become a loop downloading the first 5 pages which YouTube returns \n",
    "for page_num in range(1, 11):\n",
    "\n",
    "    # Create a search query and download the HTML code \n",
    "    query = urllib.quote(textToSearch)\n",
    "    url = \"https://www.youtube.com/results?search_query=\" + query + \"&page=\" + str(page_num)\n",
    "    response = urllib2.urlopen(url)\n",
    "    html = response.read().decode('utf-8')\n",
    "\n",
    "    # Create a BeautifulSoup object\n",
    "    soup = BeautifulSoup(html)\n",
    "\n",
    "    # Print the whole HTML code from the BeautifulSoup object\n",
    "    # print soup.prettify()[0:1000]\n",
    "\n",
    "    ## List the links to the videos and their titles\n",
    "    for vids in soup.find_all(attrs = {\"class\": \"yt-uix-tile-link\"}):\n",
    "        video_link.append(\"https://www.youtube.com\" + vids[\"href\"])\n",
    "        video_title.append(vids[\"title\"])\n",
    "\n",
    "    ## Obtain the username and channel\n",
    "    for user in soup.find_all(\"a\", attrs = {\"class\": \"g-hovercard\"}):\n",
    "        video_user.append(\"https://www.youtube.com\" + user[\"href\"])\n",
    "\n",
    "    ## Get the number of days ago a video was uploaded\n",
    "    for stats in soup.find_all(\"ul\", attrs = {\"class\": \"yt-lockup-meta-info\"}):\n",
    "        try:\n",
    "            uploaded_since.append(stats.li.get_text())\n",
    "            video_views.append(stats.li.next_sibling.get_text())\n",
    "        except:\n",
    "            uploaded_since.append(\"No Data\")\n",
    "            video_views.append(\"No Data\")\n",
    "\n",
    "\n",
    "# Add the search term that was used\n",
    "search_term = [textToSearch] * len(video_link)\n",
    "\n",
    "\n",
    "## Create a dataframe with the video links, titles, username and number of views\n",
    "final_combined_df = pd.DataFrame(list(zip(video_link, video_title, video_views, search_term)),\n",
    "                                 columns = [\"video_link\", \"video_title\", \"video_views\", \"search_term\"])\n",
    "final_combined_df.head(10)\n",
    "\n",
    "## Write to a CSV on Google Drive\n",
    "writer_excel = pd.ExcelWriter(\"C:/Users/bre49823/Google Drive/VideoSearch/videos_links_searchterm_\" + textToSearch + \".xlsx\")\n",
    "final_combined_df.to_excel(writer_excel, 'Sheet1')\n",
    "writer_excel.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning the search...\n",
      "The program will collect the information from the first 500 videos returned in the search\n",
      "The total number of videos found on YouTube for the search term amulet+biz are: 34293\n",
      "\n",
      "Stack the information about videos in a Pandas dataframe\n",
      "\n",
      "Write file to Excel on Google Drive\n",
      " The location is: C:/Users/bre49823/Google Drive/VideoSearch/videos_links_searchterm_amulet+biz.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Use the following search terms: suzanita+lucifer; suzanita+kaskata; луцифер+буда; сузанита+луцифер; amulet+biz;\n",
    "\n",
    "\n",
    "# Define the parameters\n",
    "api_key = \"AIzaSyDHXROE2Qt0NglgCmTnO8Y8cMikF5mD-m4\"\n",
    "search_term = \"amulet+biz\"\n",
    "search_pages = 10\n",
    "search_results_per_page = 50\n",
    "final_output_location = \"C:/Users/bre49823/Google Drive/VideoSearch\"\n",
    "\n",
    "\n",
    "# Initialize empty lists for the data I'll be collecting - videos' links, titles, uploader, channel, and date uploaded on\n",
    "video_link = []\n",
    "video_title = []\n",
    "video_user = []\n",
    "video_user_channel = []\n",
    "video_publish_date = []\n",
    "search_term_used = []\n",
    "\n",
    "\n",
    "# Create an empty token value for the first page - Each search page returned has a token\n",
    "# To go to the next page with results we need to add the token value to the end of the url\n",
    "# The token value will be updated from the JSON file which is returned in the API call\n",
    "next_page_token = \"\"\n",
    "\n",
    "print (\"Beginning the search...\")\n",
    "time.sleep(1)\n",
    "\n",
    "# Loop through the first n pages of search results\n",
    "for page_results in range(1, (search_pages + 1)):\n",
    "\n",
    "    # API call and JSON response\n",
    "    search_url = (\"https://www.googleapis.com/youtube/v3/search?part=snippet&maxResults=\" + str(search_results_per_page) + \"&q=\" + search_term + \"&key=\" + api_key + \"&pageToken=\" + next_page_token)\n",
    "    api_response = requests.get(url = search_url)\n",
    "    json_data = json.loads(api_response.text)\n",
    "    \n",
    "    if page_results == 1:\n",
    "        print (\"The program will collect the information from the first \" + str(search_pages * search_results_per_page) + \" videos returned in the search\")\n",
    "        time.sleep(1)\n",
    "        print (\"The total number of videos found on YouTube for the search term \" + search_term + \" are: \" + str(json_data['pageInfo']['totalResults']))\n",
    "\n",
    "    # Update the token for the next page with search results\n",
    "    next_page_token = json_data['nextPageToken']\n",
    "    \n",
    "    # Loop through all the search results returned in the API\n",
    "    for videos in range(1, len(json_data['items'])):\n",
    "        \n",
    "        # Get the link to video\n",
    "        video_link.append(\"https://www.youtube.com/watch?v=\" + json_data['items'][videos]['id'].values()[1])\n",
    "\n",
    "        # Get title of video\n",
    "        video_title.append(json_data['items'][videos]['snippet']['title'])\n",
    "\n",
    "        # Get the user name/channel name from which a video was uploaded\n",
    "        video_user.append(json_data['items'][videos]['snippet']['channelTitle'])\n",
    "\n",
    "        # Get link to the channel\n",
    "        video_user_channel.append(\"https://www.youtube.com/channel/\" + json_data['items'][videos]['snippet']['channelId'])\n",
    "    \n",
    "        # Date of video upload. The extract date is a string, from which the data is extracted\n",
    "        # Example from here: https://stackoverflow.com/questions/37192942/extract-date-from-string-in-python\n",
    "        string_date = str(json_data['items'][videos]['snippet']['publishedAt'])\n",
    "        match = re.search(r'\\d{4}-\\d{2}-\\d{2}', string_date)\n",
    "        date = datetime.strptime(match.group(), '%Y-%m-%d').date()\n",
    "        video_publish_date.append(date)\n",
    "    \n",
    "    \n",
    "# Add the search term that was used\n",
    "search_term_used = [search_term] * len(video_link)\n",
    "\n",
    "## Create a dataframe with the video links, titles, username and number of views\n",
    "print (\"\\nStack the information about videos in a Pandas dataframe\")\n",
    "time.sleep(1)\n",
    "\n",
    "final_combined_df = pd.DataFrame(list(zip(video_link, video_title, video_user, video_user_channel, video_publish_date, search_term_used)),\n",
    "                                 columns = ['video_link', 'video_title', 'video_user', 'video_user_channel', 'video_publish_date', 'search_term_used'])\n",
    "final_combined_df.head(10)\n",
    "\n",
    "\n",
    "## Write to a CSV on Google Drive\n",
    "print (\"\\nWrite file to Excel on Google Drive\\n The location is: \"  + final_output_location + \"/videos_links_searchterm_\" + search_term + \".xlsx\")\n",
    "writer_excel = pd.ExcelWriter(final_output_location + \"/videos_links_searchterm_\" + search_term + \".xlsx\")\n",
    "final_combined_df.to_excel(writer_excel, 'Sheet1')\n",
    "writer_excel.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
